It is commonly known that Recurrent Neural Networks (RNN) in theory canmaintain context over a longer period of time, but this has shown to bedifficult in practice.  This project aims to expand upon the traditionalimplementations of Long Short-Term Memory (LSTM) RNN in order toachieve a better contextual understanding over a longer period of time,compared to existing implementations.  In this report we implement severalprototypes combining an LSTM with other types of networks, with variousminor alterations and for different use cases.  The results show that increasedperformance can be gained from this approach, especially on classificationtasks, but that the inherent nature of text generation makes it hard tomeasure the performance of different networks against each others.

Created by: Einar Johnsen, Christopher Dambakk and Jonas Heer
